# Makefile for HiBench Hadoop & Spark Setup
# Usage: make <command>

.PHONY: help setup start stop restart status logs clean check build shell-spark shell-hadoop test \
	wordcount sort terasort repartition dfsioe-read dfsioe-write \
	kmeans bayes lr svm als rf gbt linear gmm lda pca xgboost svd \
	scan join aggregation \
	pagerank nutchindexing \
	nweight \
	identity repartition-streaming wordcount-streaming

# Default target
help:
	@echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
	@echo "  HiBench Hadoop & Spark Docker Setup"
	@echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
	@echo ""
	@echo "üì¶ Setup Commands:"
	@echo "  make setup        - Initialize and start everything (first time)"
	@echo "  make start        - Start containers"
	@echo "  make stop         - Stop containers"
	@echo "  make restart      - Restart all"
	@echo "  make clean        - Stop and remove everything (including volumes)"
	@echo ""
	@echo "üìä Monitoring:"
	@echo "  make status       - View container status"
	@echo "  make logs         - View logs of all services"
	@echo "  make check        - Check health of services"
	@echo ""
	@echo "üîß Development:"
	@echo "  make shell-spark  - Enter Spark Master shell"
	@echo "  make shell-hadoop - Enter Hadoop NameNode shell"
	@echo "  make test         - Run benchmark test (WordCount)"
	@echo ""
	@echo "üìä MICRO Benchmarks:"
	@echo "  make wordcount    - WordCount benchmark"
	@echo "  make sort         - Sort benchmark"
	@echo "  make terasort     - TeraSort benchmark"
	@echo "  make repartition  - Repartition benchmark"
	@echo "  make dfsioe-read  - DFSIOE Read benchmark"
	@echo "  make dfsioe-write - DFSIOE Write benchmark"
	@echo ""
	@echo "üß† MACHINE LEARNING Benchmarks:"
	@echo "  make kmeans       - K-Means clustering"
	@echo "  make bayes        - Naive Bayes"
	@echo "  make lr           - Logistic Regression"
	@echo "  make svm          - Support Vector Machine"
	@echo "  make als          - Alternating Least Squares"
	@echo "  make rf           - Random Forest"
	@echo "  make gbt          - Gradient Boosted Trees"
	@echo "  make linear       - Linear Regression"
	@echo "  make gmm          - Gaussian Mixture Model"
	@echo "  make lda          - Latent Dirichlet Allocation"
	@echo "  make pca          - Principal Component Analysis"
	@echo "  make xgboost      - XGBoost"
	@echo "  make svd          - Singular Value Decomposition"
	@echo ""
	@echo "üìä SQL Benchmarks:"
	@echo "  make scan         - Scan benchmark"
	@echo "  make join         - Join benchmark"
	@echo "  make aggregation  - Aggregation benchmark"
	@echo ""
	@echo "üåê WEB SEARCH Benchmarks:"
	@echo "  make pagerank     - PageRank algorithm"
	@echo "  make nutchindexing - Nutch Indexing"
	@echo ""
	@echo "üï∏Ô∏è  GRAPH Benchmarks:"
	@echo "  make nweight      - N-Weight graph algorithm"
	@echo ""
	@echo "‚ö° STREAMING Benchmarks:"
	@echo "  make identity              - Identity streaming"
	@echo "  make repartition-streaming - Repartition streaming"
	@echo "  make wordcount-streaming   - WordCount streaming"
	@echo ""
	@echo "üåê Web UIs:"
	@echo "  - Hadoop:  http://localhost:9870"
	@echo "  - Spark:   http://localhost:8080"
	@echo "  - Worker:  http://localhost:8081"
	@echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"

# Check if Docker is running
check:
	@echo "üîç Checking Docker..."
	@docker info > /dev/null 2>&1 || (echo "‚ùå Docker is not running!" && exit 1)
	@echo "‚úÖ Docker OK"
	@echo ""
	@echo "üîç Checking Docker Compose..."
	@which docker-compose > /dev/null || (echo "‚ùå Docker Compose is not installed!" && exit 1)
	@echo "‚úÖ Docker Compose OK"

# Initial setup (build + start + init)
setup: check
	@echo "üöÄ Starting HiBench environment setup..."
	@./scripts/setup.sh

# Build images (if needed)
build:
	@echo "üî® Building Docker images..."
	docker-compose build

# Start containers
start: check
	@echo "‚ñ∂Ô∏è  Starting containers..."
	docker-compose up -d
	@echo "‚úÖ Containers started!"
	@echo ""
	@make status

# Stop containers
stop:
	@echo "‚èπÔ∏è  Stopping containers..."
	@./scripts/stop.sh

# Restart all
restart:
	@echo "üîÑ Restarting all services..."
	docker-compose restart
	@echo "‚úÖ Services restarted!"

# View status
status:
	@./scripts/status.sh

# View logs
logs:
	docker-compose logs -f

# Logs for each service
logs-spark:
	docker-compose logs -f spark-master

logs-hadoop:
	docker-compose logs -f namenode

logs-worker:
	docker-compose logs -f spark-worker

# Enter Spark Master shell
shell-spark:
	@echo "üêö Connecting to Spark Master shell..."
	@echo "Tip: HiBench directory: /opt/hibench"
	@echo "Tip: Config files: /hibench/"
	docker exec -it spark-master bash

# Enter Hadoop NameNode shell
shell-hadoop:
	@echo "üêö Connecting to Hadoop NameNode shell..."
	docker exec -it namenode bash

# Clean up (remove everything including volumes)
clean:
	@echo "üßπ Cleaning up everything..."
	@read -p "‚ö†Ô∏è  Remove everything including HDFS data? [y/N] " -n 1 -r; \
	echo; \
	if [[ $$REPLY =~ ^[Yy]$$ ]]; then \
		docker-compose down -v; \
		echo "‚úÖ Cleaned!"; \
	else \
		echo "‚ùå Cancelled"; \
	fi

# Test with WordCount benchmark (Official HiBench)
test:
	@echo "üß™ Running Official HiBench WordCount benchmark test..."
	@bash test/hibench-official-wordcount.sh

# Quick test (only check connectivity)
test-quick:
	@echo "‚ö° Quick connectivity test..."
	@echo "Testing HDFS..."
	docker exec namenode hdfs dfs -ls /
	@echo ""
	@echo "Testing Spark..."
	docker exec spark-master spark-submit --version

# Init HDFS directories
init-hdfs:
	@echo "üìÅ Initializing HDFS directories..."
	docker exec namenode bash -c "\
		hdfs dfs -mkdir -p /HiBench && \
		hdfs dfs -mkdir -p /spark-logs && \
		hdfs dfs -mkdir -p /user/root && \
		hdfs dfs -chmod -R 777 /HiBench && \
		hdfs dfs -chmod -R 777 /spark-logs && \
		hdfs dfs -chmod -R 777 /user"
	@echo "‚úÖ HDFS initialized!"

# View HDFS
hdfs-ls:
	docker exec namenode hdfs dfs -ls /

# HDFS report
hdfs-report:
	docker exec namenode hdfs dfsadmin -report

# Clean HDFS data (keep containers)
hdfs-clean:
	@echo "üóëÔ∏è  Cleaning HiBench data on HDFS..."
	docker exec namenode hdfs dfs -rm -r -f /HiBench/* || true
	@echo "‚úÖ HDFS cleaned!"

# ============================================================================
# MICRO Benchmarks
# ============================================================================

wordcount:
	@bash scripts/run-hibench-workload.sh micro wordcount spark

sort:
	@bash scripts/run-hibench-workload.sh micro sort spark

terasort:
	@bash scripts/run-hibench-workload.sh micro terasort spark

repartition:
	@bash scripts/run-hibench-workload.sh micro repartition spark

dfsioe-read:
	@bash scripts/run-hibench-workload.sh micro dfsioe read

dfsioe-write:
	@bash scripts/run-hibench-workload.sh micro dfsioe write

# ============================================================================
# MACHINE LEARNING Benchmarks
# ============================================================================

kmeans:
	@bash scripts/run-hibench-workload.sh ml kmeans spark

bayes:
	@bash scripts/run-hibench-workload.sh ml bayes spark

lr:
	@bash scripts/run-hibench-workload.sh ml lr spark

svm:
	@bash scripts/run-hibench-workload.sh ml svm spark

als:
	@bash scripts/run-hibench-workload.sh ml als spark

rf:
	@bash scripts/run-hibench-workload.sh ml rf spark

gbt:
	@bash scripts/run-hibench-workload.sh ml gbt spark

linear:
	@bash scripts/run-hibench-workload.sh ml linear spark

gmm:
	@bash scripts/run-hibench-workload.sh ml gmm spark

lda:
	@bash scripts/run-hibench-workload.sh ml lda spark

pca:
	@bash scripts/run-hibench-workload.sh ml pca spark

xgboost:
	@bash scripts/run-hibench-workload.sh ml xgboost spark

svd:
	@bash scripts/run-hibench-workload.sh ml svd spark

# ============================================================================
# SQL Benchmarks
# ============================================================================

scan:
	@bash scripts/run-hibench-workload.sh sql scan spark

join:
	@bash scripts/run-hibench-workload.sh sql join spark

aggregation:
	@bash scripts/run-hibench-workload.sh sql aggregation spark

# ============================================================================
# WEB SEARCH Benchmarks
# ============================================================================

pagerank:
	@bash scripts/run-hibench-workload.sh websearch pagerank spark

nutchindexing:
	@bash scripts/run-hibench-workload.sh websearch nutchindexing spark

# ============================================================================
# GRAPH Benchmarks
# ============================================================================

nweight:
	@bash scripts/run-hibench-workload.sh graph nweight spark

# ============================================================================
# STREAMING Benchmarks
# ============================================================================

identity:
	@bash scripts/run-hibench-workload.sh streaming identity spark

repartition-streaming:
	@bash scripts/run-hibench-workload.sh streaming repartition spark

wordcount-streaming:
	@bash scripts/run-hibench-workload.sh streaming wordcount spark

